{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Import Required Packages\n",
    "import json\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#from pandas.io.json import json_normalize\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.linear_model import RidgeClassifierCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.svm import LinearSVR\n",
    "\n",
    "import keras\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define File Paths as Constants\n",
    "#TWITS_PATH = r\"../stocktwits_prediction/Data/twits\"\n",
    "#FINANCE_PATH = r\"../stocktwits_prediction/Data/csv\"\n",
    "TWITS_PATH = \"../Data/twits\"\n",
    "FINANCE_PATH = \"../Data/csv\"\n",
    "#Define Company Names and List of Tags as Constant\n",
    "COMPANY_NAMES = [\"apple\",\"boeing\", \"caterpillar\", \"cisco\", \"chevron\", \"dupont\", \"de\", \"nemours\", \"walt\", \n",
    "                 \"diseny\", \"facebook\", \"google\", \"vaneck\", \"goldman\", \"sachs\", \"ibm\", \"intel\", \"johnson\",\n",
    "                \"jpmorgan\", \"coca\", \"cola\", \"coca-cola\", \"mcdonalds\", \"3m\", \"merck\", \"microsoft\", \"nike\",\n",
    "                \"pfizer\", \"unitedhealth\", \"raytheon\", \"visa\", \"verizon\", \"walmart\", \n",
    "                 \"exxon\", \"mobil\"]\n",
    "\n",
    "TAGS = ['\\$FB','\\$GOOG','\\$AAPL', '\\$WB','\\$AXP','\\$BA','\\$CAT','\\$CSCO','\\$CVX','\\$DD','\\$DIS','\\$FB',\n",
    "        '\\$GE','\\$GS','\\$HD','\\$IBM','\\$INTC','\\$JNJ','\\$JPM','\\$KO','\\$MCD','\\$MMM','\\$MRK','\\$MSFT',\n",
    "        '\\$NKE','\\$PFE','\\$PG','\\$QQQ','\\$SPY','\\$TRV','\\$UNH','\\$UTX','\\$V','\\$VZ','\\$WMT','\\$XOM']\n",
    "\n",
    "LABELS = [\"Positive Return\", \"Negative Return\"]\n",
    "\n",
    "#Return ALl Files in Directory\n",
    "all_twitter = os.listdir(TWITS_PATH)\n",
    "all_finance = os.listdir(FINANCE_PATH)\n",
    "\n",
    "#Generate List of Tags\n",
    "all_tags = list()\n",
    "for file_name in all_twitter:\n",
    "    tag = '$' + file_name[:-4]\n",
    "    all_tags.append(tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stock_twits_metrics\n",
    "###  - Inputs:\n",
    "        - raw_data: DataFrame (can use returned DataFrame from the text parser\n",
    "        - n (int): number of days (used for s_t calculation)\n",
    "        - file_name (str): name of file - ex. 'FB.txt'\n",
    "###  - Returns Dataframe with Following Value Types:\n",
    "        - Date: DateTime object representing the a specific date in the sample period\n",
    "        - bearish: total count of tweets tagged with bearish sentiment on that day\n",
    "        - bullish: total count of tweets tagged with bullish sentiment on that day\n",
    "        - none: total count of tweets tagged with neither bearish nor bullisn sentiment on that day\n",
    "        - message_volume: total number of messages on that day \n",
    "        - mv1_t:  percentage change in message volume with one day period\n",
    "        - mv10_t: percentage change in message volume with ten day period\n",
    "        - polarity: (bullish - bearish)/total messages on that day\n",
    "        - s_t: moving average of polarity over n days\n",
    "        - company: string containing the abbreviation of the company name\n",
    "###  - Additional Notes:\n",
    "        - function aggregates the data into values by date - does not include the text of the orignal tweets in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stock_twits_metrics(raw_data, n, file_name):\n",
    "    #Unwrap Sentiment Values to New Columns\n",
    "    raw_data['sentiment'] = raw_data['sentiment'].apply(lambda x:{} if pd.isna(x) else x)\n",
    "    sentiment_vals = pd.json_normalize(data=raw_data['sentiment'], meta=['class','name'])\n",
    "    sentiment_vals = sentiment_vals.drop(columns='name')\n",
    "    sentiment_vals = sentiment_vals.replace(np.nan, 'none', regex=True)\n",
    "\n",
    "    #Convert Date Strings to DateTime Objects\n",
    "    raw_data['created_at'] = pd.to_datetime(raw_data['created_at']).apply(lambda x: x.date())\n",
    "\n",
    "    #Create New Dataframe\n",
    "    temp = pd.concat([raw_data['body'], sentiment_vals, raw_data['created_at']],axis=1)\n",
    "    \n",
    "    #Aggregate Text From Tweets By Date\n",
    "    agg_text = temp.groupby('created_at')['body'].apply(lambda x: x.sum())\n",
    "    agg_text.to_frame()\n",
    "    \n",
    "    #Group by Unique Dates and Extract Value Counts\n",
    "    message_volume = temp['created_at'].value_counts().rename_axis('dates').reset_index(name='message_volume')\n",
    "    message_volume = message_volume.sort_values(by='dates').reset_index(drop=True)\n",
    "\n",
    "    #Create Pivot Table With Desired Statistics\n",
    "    temp['count'] = 1\n",
    "    table = temp.pivot_table(index=['created_at'], dropna=False,\n",
    "                             columns='class',values='count',\n",
    "                             fill_value=0,aggfunc=np.sum)\n",
    "    \n",
    "    #Merge Table with Message Volume\n",
    "    table = table.reset_index()\n",
    "    table = pd.concat([table,message_volume['message_volume']],axis=1)\n",
    "\n",
    "    #Calculate Metrics and Append to the DataFrame\n",
    "    table['mv1_t'] = table['message_volume'].diff(periods=1).div(table['message_volume'].shift(1))\n",
    "    table['mv10_t'] = table['message_volume'].div(table['message_volume'].rolling(10).mean())\n",
    "\n",
    "    table['polarity'] = table.apply(lambda date: (date.bullish - date.bearish)/date.message_volume, axis = 1)\n",
    "    table['s_t'] = table['polarity'].rolling(n).mean()\n",
    "    \n",
    "    #Append Company Name to DataFrame\n",
    "    name = file_name[:-4]\n",
    "    table['company'] = name\n",
    "    \n",
    "    #Append Text to DataFrame\n",
    "    result = pd.merge(table, agg_text, on='created_at')\n",
    "    \n",
    "    #Change Column Name to Date\n",
    "    result = result.rename(columns={\"created_at\":\"Date\"})\n",
    "    \n",
    "    result = result.dropna(how = 'any')\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stock_twits_text_parser\n",
    "###  - Inputs:\n",
    "        - file (str): name of file - ex. 'FB.txt'\n",
    "###  - Returns Dataframe with Following Value Types:\n",
    "        - df: DataFrame with all rows containing multiple tagged companies dropped, all columns included - index reset to reflect new size\n",
    "###  - Additional Notes:\n",
    "        - None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stock_twits_text_parser(file):\n",
    "    #Read File\n",
    "    file_path = TWITS_PATH + '/' + file\n",
    "    raw_data = pd.read_json(file_path)\n",
    "\n",
    "    #Create New Column with List of Unique Tags\n",
    "    raw_data[\"instances\"] = (raw_data['body'].str.findall(f'(?i)({\"|\".join(TAGS)})')\n",
    "                                .apply(lambda x: list(set(map(str.upper,x))))\n",
    "                            )\n",
    "    #Drop All Rows With Multiple Tags and Reset Index\n",
    "    df = raw_data[raw_data['instances'].map(lambda x: len(x)) < 2]\n",
    "    df = df.reset_index()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feature_selector\n",
    "###  - Inputs:\n",
    "        - metrics (DataFrame): DataFrame containing the body text of the tweet eg. data['body'] along with other metrics\n",
    "        - labels (DataFrame) DataFrame containing the target labels\n",
    "        - target_label (str): name of the column containing the target label we want to use\n",
    "        - min_count(int) : minimum number of occurances so that the word is converted\n",
    "        - n (int): number of text features selected\n",
    "###  - Returns Dataframe with Following Value Types:\n",
    "        - df: DataFrame with top n text features and all numerical features, raw data and labels (20 + n columns total)\n",
    "###  - Additional Notes:\n",
    "        - text features will always be appended after the numerical metrics, raw data and labels - currently these are the first 20 columns: use df.iloc[:,20:] to extract only text features for analysis\n",
    "        - text features selected using chi squared statistics\n",
    "        - ***TODO: Currently the method selects features based on categorical outcomes (classification problem) this will need to be changed to allow for regression (explore a way to bin continuous target data) ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selector(metrics, labels, target_label, min_count, n):\n",
    "    #Merge DataFranes\n",
    "    result = pd.merge(metrics, labels, on='Date')\n",
    "    \n",
    "    #Remove Company Names By Adding Them To Stop Words\n",
    "    new_stop_words = text.ENGLISH_STOP_WORDS.union(COMPANY_NAMES)\n",
    "\n",
    "    #Vectorizer Parameters: Convert To Lowercase, Remove Stop Words With Company Names,Min Count 25, Laplace Smoothing\n",
    "    v = TfidfVectorizer(analyzer='word', lowercase=True,stop_words=new_stop_words, min_df = min_count, smooth_idf=True)\n",
    "    x = v.fit_transform(result['body'])\n",
    "\n",
    "    #Convert back to DataFrame\n",
    "    text_data = pd.DataFrame(x.toarray(),columns=v.get_feature_names())\n",
    "    \n",
    "    #Select n Best Features\n",
    "    X = text_data\n",
    "    if(X.shape[1] >= n):\n",
    "        Y = convert_to_binary_classification(result[target_label])\n",
    "        chi2_selector = SelectKBest(chi2, k=n)\n",
    "        X_best = chi2_selector.fit_transform(X,Y)\n",
    "    else:\n",
    "        X_best = X\n",
    "        print(\"Warning: found max of \" + str(X.shape[1]) + \" text features - all features used\")\n",
    "              \n",
    "    data = pd.DataFrame(X_best)\n",
    "    df = pd.concat([result,data],axis=1)\n",
    "    \n",
    "    df = df.dropna()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# return_over_period_T\n",
    "###  - Inputs:\n",
    "        - file (str): name of file - ex. 'FB.csv'\n",
    "        - T (int): period based on which return is calculated (in days)\n",
    "###  - Returns Dataframe with Following Value Types:\n",
    "        - Date: DateTime object representing a specific date in the sample period\n",
    "        - OPEN, HIGH, LOW, VOLUME, CLOSE: prices each day with respect to the specific metric\n",
    "        - xxx_return: return for referenced metric over period T, where T is specified when the function is called\n",
    "###  - Additional Notes:\n",
    "        - function includes original data taken from the CSV file\n",
    "        - formula for return over period T is defined as: r(T) = (p(t+T) - p(t))/p(t), where t is current day and p(t) i the price at the current day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_over_period_T(file, T):\n",
    "    #Read File\n",
    "    file_path = FINANCE_PATH + '/' + file\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    #Convert Date Strings to DateTime Objects\n",
    "    df['Date'] = pd.to_datetime(df['Date']).apply(lambda x: x.date())\n",
    "\n",
    "    #Calculate Metrics for OPEN, HIGH, LOW, VOLUME and CLOSE\n",
    "    df['open_return'] = -df['OPEN'].diff(periods = -T).div(df['OPEN'])\n",
    "    df['high_return'] = -df['HIGH'].diff(periods = -T).div(df['HIGH'])\n",
    "    df['low_return'] = -df['LOW'].diff(periods = -T).div(df['LOW'])\n",
    "    df['close_return'] = -df['CLOSE'].diff(periods = -T).div(df['CLOSE'])\n",
    "    \n",
    "    df = df.dropna(how='any')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# convert_to_binary_classification\n",
    "###  - Inputs:\n",
    "        - df (DataFrame): single column to be classified\n",
    "###  - Returns Dataframe with Following Value Types:\n",
    "        - Binary Labels: single column of binary labels representing positive and negative returns on the day\n",
    "###  - Additional Notes:\n",
    "        - Note that a negative return is defined as class 0 and a postive return or no return is defined as class 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_binary_classification(df):\n",
    "    binary_classification = df.apply(lambda x: 0 if x < 0 else 1)\n",
    "    return binary_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# split_data\n",
    "###  - Inputs:\n",
    "        - df (DataFrame): Processed DataFrame with features and labels to be split\n",
    "        - train_percentage (float): Percentage of data to be used as training data - assumes all other data is used as test\n",
    "        - features (str): toggles whether to use text only or text and numerical features (valid inputs: 'text', 'all')\n",
    "        - label (str): selects target label based on column name\n",
    "        - target_type (str): binary (bin) or continuous (cont)\n",
    "###  - Returns Dataframe with Following Value Types:\n",
    "        - data_dict (Dictionary): dictionary with 4 values corresponding to X_train, X_test, Y_train, Y_test (keys)\n",
    "###  - Additional Notes:\n",
    "        - ***TODO: does not handle continous data correctly***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df, train_percentage, features, label, target_type):\n",
    "    #Determine Split Index Based on the Percentage Of Data to Be Used For Training\n",
    "    train_index = int(train_percentage*len(df.index)) + 1\n",
    "    \n",
    "    #Split Data\n",
    "    if(features == 'text'):\n",
    "        X_train = df.iloc[0:train_index,20:]\n",
    "        X_test = df.iloc[train_index:,20:]\n",
    "    elif(features == 'all'):\n",
    "        df1 = df.iloc[0:train_index:,np.r_[5:7,8]]\n",
    "        df2 = df.iloc[0:train_index,20:]\n",
    "        X_train = pd.concat([df1,df2],axis=1)\n",
    "        \n",
    "        df3 = df.iloc[train_index:,np.r_[5:7,8]]\n",
    "        df4 = df.iloc[train_index:,20:]\n",
    "        X_test = pd.concat([df3,df4],axis=1)\n",
    "    else:\n",
    "        raise ValueError('Invalid Parameter Input - features')\n",
    "        \n",
    "    if(target_type == 'bin' or target_type == 'cont'):\n",
    "        if(target_type == 'bin'):\n",
    "            Y = convert_to_binary_classification(df[label])\n",
    "        else:\n",
    "            Y = df[label]\n",
    "        Y_train = Y.iloc[0:train_index]\n",
    "        Y_test = Y.iloc[train_index:]\n",
    "    else:\n",
    "        raise ValueError('Invalid Parameter Input - target_type')\n",
    "    \n",
    "    #Return as a Dictionary\n",
    "    data_dict = dict()\n",
    "    \n",
    "    data_dict['X_train'] = X_train\n",
    "    data_dict['X_test'] = X_test\n",
    "    data_dict['Y_train'] = Y_train\n",
    "    data_dict['Y_test'] = Y_test\n",
    "    \n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create_metrics_table\n",
    "###  - Inputs:\n",
    "        - Y_train: training labels\n",
    "        - pred_train: predicted labels from training data\n",
    "        - Y_test: training labels\n",
    "        - pred_test: predicted labels from test data\n",
    "        - valid input types (series, numpy array, DF columns) - must be column vectors\n",
    "###  - Returns Dataframe with Following Value Types:\n",
    "        - table (DataFrame): table with REC, PREC, F1 amd ACC for training and test predictions\n",
    "###  - Additional Notes:\n",
    "        - None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_metrics_table(Y_train, pred_train, Y_test, pred_test):\n",
    "    #Calculate Metrics\n",
    "    REC_train = recall_score(Y_train,pred_train)\n",
    "    PREC_train = precision_score(Y_train, pred_train)\n",
    "    F1_train = f1_score(Y_train, pred_train)\n",
    "    ACC_train = accuracy_score(Y_train, pred_train)\n",
    "\n",
    "    REC_test = recall_score(Y_test,pred_test)\n",
    "    PREC_test = precision_score(Y_test,pred_test)\n",
    "    F1_test = f1_score(Y_test, pred_test)\n",
    "    ACC_test = accuracy_score(Y_test, pred_test)\n",
    "\n",
    "    #Create Table\n",
    "    rows = [[\"Training\", REC_train, PREC_train, F1_train, ACC_train], [\"Test\", REC_test, PREC_test, F1_test,ACC_test]]\n",
    "    table = pd.DataFrame(rows, columns = [\"Dataset\", \"Recall\", \"Precision\", \"F1 Score\", \"Accuracy\"])\n",
    "    table.set_index(\"Dataset\", inplace =True)\n",
    "\n",
    "    return table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot_confusion_matrix\n",
    "###  - Inputs:\n",
    "        - matrix (confusion matrix): confusion matrix generated from sklearn confusion_matrix\n",
    "        - data_set_name (str): name of dataset ex. 'training' or 'test' \n",
    "###  - Returns Dataframe with Following Value Types:\n",
    "        - None - void type function\n",
    "        - Displays plot\n",
    "###  - Additional Notes:\n",
    "        - Wrapper for seaborn plot code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(matrix, data_set_name):\n",
    "    sns.heatmap(matrix, annot=True, fmt= \".3f\", xticklabels = LABELS, yticklabels = LABELS)\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"Actual Label\")\n",
    "    plt.title(\"Confusion Matrix for \" + data_set_name + \" Set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot_ROC_curve\n",
    "###  - Inputs:\n",
    "        - FPR: False Positive Rate - Generated from sklearn roc_curve\n",
    "        - TPR: True Positive Rate - Generated from sklearn roc_curve\n",
    "        - data_set_name (str): name of dataset ex. 'training' or 'test'\n",
    "###  - Returns Dataframe with Following Value Types:\n",
    "        - None - void type function\n",
    "        - Displays plot\n",
    "###  - Additional Notes:\n",
    "        - Wrapper for matplotlib plot code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ROC_curve(FPR, TPR, data_set_name):\n",
    "    plt.plot([0,1],[0,1],'k--')\n",
    "    plt.plot(FPR, TPR)\n",
    "    plt.xlabel(\"False Positive\")\n",
    "    plt.ylabel(\"True Positive\")\n",
    "    plt.title(\"ROC Curve for \" + data_set_name + \" Data\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get_best_SVC\n",
    "###  - Inputs:\n",
    "        - model_type (str): toggles between selecting the best linear model or choosing from all types - valid inputs ('all', 'linear')\n",
    "        - alphas (list): list containing the distinct values of the regularization parameter to be chosen between\n",
    "        - folds (int): number of folds used for cross validation\n",
    "        - X_train (DataFrame): training feature data\n",
    "        - Y_train (DataFrame): training label data\n",
    "        - verbose (int): 0 to hide chosen parameters, 1 to show chosen parameters\n",
    "###  - Returns Dataframe with Following Value Types:\n",
    "        - best_model (estimator object): returns estimator selected based on the given parameters\n",
    "###  - Additional Notes:\n",
    "        - prints the parameters selected to generate the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_SVC(model_type, alphas, folds, X_train, Y_train, verbose):\n",
    "    if(model_type == 'linear'):\n",
    "        lin_svc = LinearSVC(dual=False ,max_iter=500000)\n",
    "        params = {'penalty':['l1','l2'],'C':alphas}\n",
    "        clf = GridSearchCV(estimator=lin_svc, param_grid=params, cv=folds)\n",
    "        clf.fit(X_train,Y_train)\n",
    "        \n",
    "        if(verbose == 1):\n",
    "            print(\"Best Parameters: \")\n",
    "            print(clf.best_params_)\n",
    "        \n",
    "        best_model = clf.best_estimator_\n",
    "    elif(model_type == 'all'):\n",
    "        svc = SVC(max_iter=500000)\n",
    "        params = {'C':alphas, 'kernel':['linear','poly','sigmoid','rbf']}\n",
    "        clf = GridSearchCV(estimator=svc, param_grid=params, cv=folds)\n",
    "        clf.fit(X_train,Y_train)\n",
    "        \n",
    "        if(verbose == 1):\n",
    "            print(\"Best Parameters: \")\n",
    "            print(clf.best_params_)\n",
    "        \n",
    "        best_model = clf.best_estimator_\n",
    "    else:\n",
    "        raise ValueError('Invalid Parameter Input - model_type')\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build_neural_network_cf\n",
    "###  - Inputs:\n",
    "        - features (int): number of features - default none (will throw error if int not provided)\n",
    "###  - Returns Dataframe with Following Value Types:\n",
    "        - model (keras estimator): compiled estimator specified by the function\n",
    "###  - Additional Notes:\n",
    "        - Neural Network strucutre needs to be tuned by hand - function does not handle this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_neural_network_cf(features=None):\n",
    "    #Build Model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, input_dim=features, activation='relu'))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    #Compile Model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get_best_knn_regressor\n",
    "###  - Inputs:\n",
    "        - lower_bound_k (int): lower bound of k values being selected from (inclusive)\n",
    "        - upper_bound_k (int): upper bound of k values being selected from (inclusive)\n",
    "        - step_size (int): step size between values of k in specified range [lower_bound_k, upper_bound_k]\n",
    "        - folds (int): number of folds used for cross validation\n",
    "        - X_train (DataFrame): training feature data\n",
    "        - Y_train (DataFrame): training label data\n",
    "###  - Returns Dataframe with Following Value Types:\n",
    "        - best_model (estimator object): returns estimator selected based on the given parameters\n",
    "###  - Additional Notes:\n",
    "        - prints the parameters selected to generate the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
=======
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_knn_regressor(lower_bound_k, upper_bound_k, step_size, folds, X_train, Y_train):\n",
    "    #Create List of Possible K Values\n",
    "    k_vals = list()\n",
    "    while(lower_bound_k <= upper_bound_k):\n",
    "        k_vals.append(lower_bound_k)\n",
    "        lower_bound_k+=step_size\n",
    "    \n",
    "    #Define Parameters to Cross-Validate\n",
    "    params = {'n_neighbors':k_vals,'weights':['uniform','distance'],'p':[1,2,3]}\n",
    "    \n",
    "    #Select Best Parameters\n",
    "    knn = KNeighborsRegressor()\n",
    "\n",
    "    knn_reg = GridSearchCV(estimator=knn, param_grid=params, cv=folds)\n",
    "    knn_reg.fit(X_train,Y_train)\n",
    "        \n",
    "    print(\"Best Parameters: \")\n",
    "    print(knn_reg.best_params_)\n",
    "        \n",
    "    best_model = knn_reg.best_estimator_\n",
    "    \n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get_best_svr"
=======
    "# all_models_cf\n",
    "###  - Inputs:\n",
    "        - file_name (str): name of file - ex. 'FB.txt'\n",
    "        - csv_name (str): name of csv - ex. 'FB.csv'\n",
    "        - features (int): number of text features to include\n",
    "        - verbose (int): 0 to hide messages, 1 to show messages\n",
    "###  - Returns Dataframe with Following Value Types:\n",
    "        - result (list): list containing the name of the dataset, best model type and corresponding accuracy\n",
    "###  - Additional Notes:\n",
    "        - none"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_models_cf(file_name, csv_name, features, verbose):\n",
    "    all_features = features+3\n",
    "    name = file_name[:-4]\n",
    "    \n",
    "    company = stock_twits_text_parser(file_name)\n",
    "    company_metrics = stock_twits_metrics(company, 3, file_name)\n",
    "    company_return = return_over_period_T(csv_name,3)\n",
    "    agg_data = feature_selector(company_metrics, company_return, 'close_return', 25, features)\n",
    "    \n",
    "    if(features > agg_data.iloc[:,20:].shape[1]):\n",
    "        all_features =agg_data.iloc[:,20:].shape[1] + 3\n",
    "    \n",
    "    company_data = split_data(agg_data, 0.7, 'all', 'close_return', 'bin')\n",
    "\n",
    "    #Split\n",
    "    X_train = company_data['X_train']\n",
    "    Y_train = company_data['Y_train']\n",
    "    X_test = company_data['X_test']\n",
    "    Y_test = company_data['Y_test']\n",
    "\n",
    "    reg_vals = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4, 1e5]\n",
    "\n",
    "    clf_lin_svc =  get_best_SVC('linear', reg_vals, 5, X_train, Y_train,0)\n",
    "\n",
    "    model = KerasClassifier(build_fn=build_neural_network_cf, features=all_features, epochs=100, batch_size=10, verbose=0)\n",
    "\n",
    "    best_accuracy = clf_lin_svc.score(X_test,Y_test)\n",
    "    best_classifier = 0\n",
    "\n",
    "    l1 = Pipeline([('clf_l1',LogisticRegressionCV(Cs=[1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4, 1e5], cv = 5, penalty = \"l1\", solver = 'liblinear'))])\n",
    "    l2 = Pipeline([('clf_l2',RidgeClassifierCV(alphas=[1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4, 1e5], cv = 5))])\n",
    "    rf = Pipeline([('clf_rf',RandomForestClassifier())])\n",
    "    et = Pipeline([('clf_et',ExtraTreesClassifier())])\n",
    "    nn = Pipeline([('clf_nn',model)])\n",
    "\n",
    "    pipe_dict = {0:'SVC',1:'L1',2:\"L2\",3:\"Random Forest\",4:\"Extra Trees\",5:\"Neural Network\"}\n",
    "    pipelines =[l1,l2,rf,et,nn]\n",
    "\n",
    "    for pipe in pipelines:\n",
    "        pipe.fit(X_train, Y_train)\n",
    "    \n",
    "    if(verbose == 1):\n",
    "        print(\"SVC Test Accuracy: \" + str(best_accuracy))\n",
    "        for i, model in enumerate(pipelines):\n",
    "            print(\"{} Test Accuracy: {}\".format(pipe_dict[i+1],model.score(X_test,Y_test)))\n",
    "\n",
    "    for i, model in enumerate(pipelines):\n",
    "        if(model.score(X_test, Y_test)>best_accuracy):\n",
    "            best_accuracy = model.score(X_test,Y_test)\n",
    "            best_classifier = i+1\n",
    "        \n",
    "    result = [name, pipe_dict[best_classifier], best_accuracy]\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  - Inputs:\n",
    "        - model_type (str): toggles between selecting the best linear model or choosing from all types - valid inputs ('all', 'linear')\n",
    "        - alphas (list): list containing the distinct values of the regularization parameter to be chosen between\n",
    "        - degrees (list) - different degrees of the polynomial kernel function to be chosen from if 'poly' is selected. Ignored by all other kernels \n",
    "        - folds (int): number of folds used for cross validation\n",
    "        - X_train (DataFrame): training feature data\n",
    "        - Y_train (DataFrame): training label data\n",
    "###  - Returns Dataframe with Following Value Types:\n",
    "        - best_model (estimator object): returns estimator selected based on the given parameters\n",
    "###  - Additional Notes:\n",
    "        - prints the parameters selected to generate the best model"
=======
    "# all_models_cf\n",
    "###  - Inputs:\n",
    "        - companies (list) - list of company abbreviations eg. FB, AXP, CAT\n",
    "        - n_features (int) - number of text features to include\n",
    "###  - Returns Dataframe with Following Value Types:\n",
    "        - table (DataFrame) - table of results containing the name of each stock, best estimator and the corresponding accuracy\n",
    "###  - Additional Notes:\n",
    "        - none"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_SVR(model_type, alphas, degrees, folds, X_train, Y_train):\n",
    "    if(model_type == 'linear'):\n",
    "        lin_svr = LinearSVR(dual = False, max_iter=500000)\n",
    "        params = {'loss':['squared_epsilon_insensitive'],'C':alphas, 'fit_intercept': [True, False]}\n",
    "        clf = GridSearchCV(estimator=lin_svr, param_grid=params, cv=folds)\n",
    "        clf.fit(X_train,Y_train)\n",
    "        \n",
    "        print(\"Best Parameters: \")\n",
    "        print(clf.best_params_)\n",
    "        \n",
    "        best_model = clf.best_estimator_\n",
    "    elif(model_type == 'all'):\n",
    "        svr = SVR(max_iter=500000)\n",
    "        params = {'C':alphas, 'kernel':['linear','poly','sigmoid','rbf'], 'degree': degrees }   #'gamma': ['scale','auto']\n",
    "        clf = GridSearchCV(estimator=svr, param_grid=params, cv=folds)\n",
    "        clf.fit(X_train,Y_train)\n",
    "        \n",
    "        print(\"Best Parameters: \")\n",
    "        print(clf.best_params_)\n",
    "        print(\"Ignore degree value if kernel is not 'poly' \")\n",
    "        best_model = clf.best_estimator_\n",
    "    else:\n",
    "        raise ValueError('Invalid Parameter Input - model_type')\n",
    "    return best_model"
=======
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_datasets_cf(companies, n_features):\n",
    "    company_performance = list()\n",
    "    for company in companies:\n",
    "        file_name = company + '.txt'\n",
    "        csv = company + '.csv'\n",
    "        features = n_features\n",
    "        result = all_models_cf(file_name, csv, features, 0)\n",
    "        company_performance.append(result)\n",
    "        \n",
    "    #Create Table\n",
    "    rows = company_performance\n",
    "    table = pd.DataFrame(rows, columns = [\"Dataset\", \"Best Model\", \"Test Score\"])\n",
    "    table.set_index(\"Dataset\", inplace =True)\n",
    "        \n",
    "    return table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "file_name = 'FB.json'\n",
    "csv_name = 'FB.csv'\n",
    "\n",
    "company = stock_twits_text_parser(file_name)\n",
    "company_metrics = stock_twits_metrics(company, 3, file_name)\n",
    "company_return = return_over_period_T(csv_name,3)\n",
    "agg_data = feature_selector(company_metrics, company_return, 'close_return', 25, 500)\n",
=======
    "#file_name = 'AXP.txt'\n",
    "#csv_name = 'AXP.csv'\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
=======
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#company_data = split_data(agg_data, 0.7, 'all', 'close_return', 'bin')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'X_train':         mv1_t    mv10_t       s_t         0         1         2         3  \\\n",
       " 0    0.052830  0.982394  0.305705  0.018151  0.000000  0.000000  0.017061   \n",
       " 1    0.060932  1.029207  0.328994  0.044150  0.000000  0.000000  0.005533   \n",
       " 2    4.560000  1.730290  0.295255  0.027149  0.000000  0.000000  0.031407   \n",
       " 3    0.412470  2.008183  0.266393  0.021485  0.003029  0.000000  0.016156   \n",
       " 4    4.066214  5.075693  0.222265  0.009036  0.002730  0.000649  0.004247   \n",
       " ..        ...       ...       ...       ...       ...       ...       ...   \n",
       " 233  0.526316  1.626721  0.231857  0.017109  0.000000  0.024583  0.000000   \n",
       " 234  7.219512  1.920228  0.202075  0.005289  0.000000  0.053200  0.000000   \n",
       " 235 -0.059347  1.557740  0.276500  0.011161  0.005900  0.050516  0.000000   \n",
       " 236 -0.485804  0.753235  0.258743  0.019317  0.000000  0.068004  0.009079   \n",
       " 237  0.030675  0.783582  0.256936  0.000000  0.000000  0.050651  0.000000   \n",
       " \n",
       "             4         5         6  ...       490       491       492  \\\n",
       " 0    0.000000  0.000000  0.082916  ...  0.004886  0.000000  0.000000   \n",
       " 1    0.000000  0.000000  0.074220  ...  0.000000  0.000000  0.000000   \n",
       " 2    0.000000  0.004059  0.043502  ...  0.003373  0.012817  0.000000   \n",
       " 3    0.002628  0.000000  0.059675  ...  0.000000  0.000000  0.000000   \n",
       " 4    0.001777  0.000000  0.065813  ...  0.000000  0.000990  0.001009   \n",
       " ..        ...       ...       ...  ...       ...       ...       ...   \n",
       " 233  0.004485  0.000000  0.013398  ...  0.043418  0.007500  0.007641   \n",
       " 234  0.000000  0.000000  0.011598  ...  0.034168  0.000000  0.000000   \n",
       " 235  0.010240  0.005422  0.015295  ...  0.040554  0.000000  0.000000   \n",
       " 236  0.008862  0.000000  0.015884  ...  0.132586  0.014820  0.000000   \n",
       " 237  0.000000  0.000000  0.011042  ...  0.081326  0.015453  0.000000   \n",
       " \n",
       "           493       494       495       496       497       498       499  \n",
       " 0    0.006713  0.013836  0.006583  0.006748  0.000000  0.000000  0.004306  \n",
       " 1    0.016329  0.000000  0.006405  0.000000  0.008579  0.000000  0.000000  \n",
       " 2    0.004634  0.004776  0.027266  0.000000  0.000000  0.008365  0.002973  \n",
       " 3    0.006357  0.008189  0.034285  0.003195  0.000000  0.005738  0.002039  \n",
       " 4    0.007520  0.004797  0.023877  0.000720  0.000941  0.000646  0.005513  \n",
       " ..        ...       ...       ...       ...       ...       ...       ...  \n",
       " 233  0.013559  0.002795  0.000000  0.005452  0.000000  0.000000  0.000000  \n",
       " 234  0.002934  0.006048  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       " 235  0.006192  0.012761  0.000000  0.000000  0.008133  0.000000  0.003972  \n",
       " 236  0.005358  0.011044  0.000000  0.000000  0.000000  0.000000  0.013748  \n",
       " 237  0.011175  0.000000  0.000000  0.000000  0.000000  0.010086  0.000000  \n",
       " \n",
       " [238 rows x 503 columns],\n",
       " 'X_test':         mv1_t    mv10_t       s_t         0         1         2         3  \\\n",
       " 238 -0.160714  0.710327  0.195120  0.000000  0.000000  0.011618  0.021715   \n",
       " 239  2.181818  0.782998  0.299134  0.007335  0.015510  0.000000  0.082730   \n",
       " 240  1.476190  1.666667  0.320013  0.020928  0.000000  0.021050  0.045900   \n",
       " 241 -0.276923  1.332388  0.232328  0.017598  0.000000  0.000000  0.115789   \n",
       " 242  0.351064  1.884273  0.244064  0.000000  0.000000  0.000000  0.039706   \n",
       " ..        ...       ...       ...       ...       ...       ...       ...   \n",
       " 334  0.643216  1.570228  0.108464  0.007700  0.002714  0.002582  0.004825   \n",
       " 335 -0.366972  0.954357  0.154545  0.008158  0.004313  0.004103  0.003834   \n",
       " 336  0.429952  1.279723  0.168350  0.010822  0.000000  0.000000  0.000000   \n",
       " 337  5.115385  1.957164  0.299279  0.007767  0.000000  0.002232  0.002086   \n",
       " 338 -0.523270  0.875693  0.318780  0.011179  0.000000  0.000000  0.004203   \n",
       " \n",
       "             4         5         6  ...       490  491       492       493  \\\n",
       " 238  0.000000  0.000000  0.006332  ...  0.074619  0.0  0.000000  0.025633   \n",
       " 239  0.000000  0.000000  0.000000  ...  0.118453  0.0  0.000000  0.008138   \n",
       " 240  0.000000  0.000000  0.030593  ...  0.073231  0.0  0.000000  0.003870   \n",
       " 241  0.008073  0.000000  0.000000  ...  0.120787  0.0  0.013754  0.024408   \n",
       " 242  0.025838  0.034207  0.000000  ...  0.079592  0.0  0.011005  0.003906   \n",
       " ..        ...       ...       ...  ...       ...  ...       ...       ...   \n",
       " 334  0.002355  0.004988  0.009849  ...  0.076683  0.0  0.000000  0.008544   \n",
       " 335  0.000000  0.003963  0.004472  ...  0.085632  0.0  0.000000  0.004526   \n",
       " 336  0.002837  0.000000  0.003390  ...  0.064918  0.0  0.000000  0.017154   \n",
       " 337  0.000000  0.000000  0.004866  ...  0.073466  0.0  0.000000  0.014773   \n",
       " 338  0.000000  0.000000  0.000000  ...  0.108326  0.0  0.000000  0.000000   \n",
       " \n",
       "           494      495       496       497       498       499  \n",
       " 238  0.006604  0.00000  0.000000  0.016835  0.011568  0.016442  \n",
       " 239  0.008387  0.00000  0.000000  0.000000  0.000000  0.000000  \n",
       " 240  0.011965  0.00000  0.000000  0.000000  0.013972  0.000000  \n",
       " 241  0.000000  0.00000  0.000000  0.000000  0.000000  0.000000  \n",
       " 242  0.012075  0.00766  0.000000  0.000000  0.000000  0.000000  \n",
       " ..        ...      ...       ...       ...       ...       ...  \n",
       " 334  0.007337  0.00000  0.002862  0.000000  0.002570  0.009133  \n",
       " 335  0.000000  0.00000  0.000000  0.000000  0.000000  0.011612  \n",
       " 336  0.007071  0.00000  0.000000  0.000000  0.003097  0.002201  \n",
       " 337  0.005075  0.00000  0.000000  0.000000  0.000000  0.007896  \n",
       " 338  0.002557  0.00000  0.000000  0.000000  0.004478  0.019095  \n",
       " \n",
       " [101 rows x 503 columns],\n",
       " 'Y_train': 0      0\n",
       " 1      1\n",
       " 2      1\n",
       " 3      0\n",
       " 4      0\n",
       "       ..\n",
       " 233    1\n",
       " 234    1\n",
       " 235    1\n",
       " 236    1\n",
       " 237    1\n",
       " Name: close_return, Length: 238, dtype: int64,\n",
       " 'Y_test': 238    1\n",
       " 239    1\n",
       " 240    1\n",
       " 241    1\n",
       " 242    0\n",
       "       ..\n",
       " 334    1\n",
       " 335    1\n",
       " 336    1\n",
       " 337    0\n",
       " 338    0\n",
       " Name: close_return, Length: 101, dtype: int64}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#company_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
